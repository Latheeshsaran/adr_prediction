{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfr1XTEBY0h1"
      },
      "source": [
        "Downloading Model weights and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaQ4_JJ0WkPS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/lovelindhoni/adr_prediction.git\n",
        "!mv adr_prediction/data .\n",
        "!mv adr_prediction/trained_weights .\n",
        "!rm -rf adr_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4uKOA3acMRt"
      },
      "source": [
        "Install the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqZd7U7HzmfL"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install onednn-cpu-gomp\n",
        "!pip install --upgrade intel-extension-for-tensorflow[cpu]\n",
        "!pip install pandas\n",
        "!pip install scikit-learn-intelex\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzM8mIOZcb0o"
      },
      "source": [
        "Importing all the necessary packages so we can reuse it on multiple cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awXmexSS-O6f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import intel_extension_for_tensorflow as itex\n",
        "import keras\n",
        "import pandas as pd\n",
        "from keras.layers import *\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from keras.regularizers import *\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "from sklearn.metrics import roc_curve, average_precision_score, roc_auc_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-hYWeZnXIIE"
      },
      "source": [
        "Enabling OneDNN optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5zxvpQ2dZrG"
      },
      "source": [
        "Checking tensorflow version and the availability of gpu support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH7V9dUTds7O",
        "outputId": "59c8abe5-c57f-4ede-8375-ba603624fe67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.15.0 2.15.0\n",
            "GPU Available: []\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__, keras.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEmV_HYqX8_0"
      },
      "source": [
        "Configuring oneDNN to use GPU acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ib7LkDD3X7vg"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
        "os.environ['DNNL_ENGINE_LIMIT_CPU_CAPABILITIES'] = '0'\n",
        "os.environ['ONEDNN_VERBOSE'] ='0'\n",
        "os.environ['SYCL_DEVICE_FILTER'] = 'opencl:gpu'\n",
        "os.environ['SYCL_ENABLE_DEFAULT_CONTEXTS'] = '1'\n",
        "os.environ['SYCL_ENABLE_FUSION_CACHING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -c \"import intel_extension_for_tensorflow as itex; print(itex.__version__)\"\n",
        "os.environ['ITEX_XPU_BACKEND'] = 'CPU'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZCRSrMjXYx-"
      },
      "source": [
        "CustomModelCheckpoint is a custom callback which saves the best modal on training and adjusts the learning rate based on decay properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T_oA0EdsbeQj"
      },
      "outputs": [],
      "source": [
        "class CustomModelCheckPoint(keras.callbacks.Callback): # custom model checkpoint\n",
        "    def __init__(self, save_path, model_name, init_learining_rate, decay_rate, decay_steps,\n",
        "                 save_best_metric='val_loss',this_max=False, **kargs):\n",
        "        super(CustomModelCheckPoint,self).__init__(**kargs)\n",
        "        self.epoch_loss = {}\n",
        "        self.epoch_val_loss = {}\n",
        "        self.save_path = save_path\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.init_learining_rate = init_learining_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.global_step = 0\n",
        "\n",
        "        self.save_best_metric = save_best_metric\n",
        "        self.max = this_max\n",
        "        if this_max:\n",
        "            self.best = float('-inf')\n",
        "        else:\n",
        "            self.best = float('inf')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      # at the beginning of each epoch if the calculated metric is better, then we save that model state with the better metric value.\n",
        "        metric_value = logs.get(self.save_best_metric)\n",
        "        if self.max:\n",
        "            if metric_value > self.best:\n",
        "                self.best = metric_value\n",
        "                self.best_model = self.model\n",
        "        else:\n",
        "            if metric_value < self.best:\n",
        "                self.best = metric_value\n",
        "                self.best_model = self.model\n",
        "\n",
        "        self.epoch_loss[epoch] = logs.get('loss')\n",
        "        self.epoch_val_loss[epoch] = logs.get('val_loss')\n",
        "        self.best_model.save_weights(self.save_path + self.model_name + '.h5')\n",
        "\n",
        "    def on_epoch_begin(self, epoch, _):\n",
        "        # at the beginning of each epoch, we modify the learning rate with respect to decay rate.\n",
        "        actual_lr = float(K.get_value(self.model.optimizer.lr))\n",
        "        decayed_learning_rate = actual_lr * self.decay_rate ** (epoch / self.decay_steps)\n",
        "        K.set_value(self.model.optimizer.lr, decayed_learning_rate)\n",
        "        if epoch % 10 == 0: # additionally at the begining of each cycle of 10 epoch we reset the learning rate to the initial value.\n",
        "            K.set_value(self.model.optimizer.lr, self.init_learining_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C4ttVh6kRU4"
      },
      "source": [
        "CustomDataGenerator generates batches of processed data for the model by adjusting the batch size dynamically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZiX7tIZ4beQk"
      },
      "outputs": [],
      "source": [
        "def find_exp(drug_df, ts_exp, column_name):\n",
        "    return pd.merge(drug_df, ts_exp, left_on=column_name, right_on='pubchem', how='left').iloc[:,2:]\n",
        "\n",
        "class CustomDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, x_set, y_label, batch_size, exp_df, shuffle=True):\n",
        "        self.x = x_set\n",
        "        self.y = y_label\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.x))\n",
        "        self.shuffle = shuffle\n",
        "        self.exp_df = exp_df\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self): # shuffling at the end of each epoch\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __len__(self): # length of batch\n",
        "        return math.ceil(len(self.x)/self.batch_size)\n",
        "\n",
        "    def __data_generation__(self, x_list):\n",
        "      # x_list has the data of drug1, drug2 and sideEffects.\n",
        "        x1 = find_exp(x_list[['drug1']], self.exp_df, 'drug1')\n",
        "        x2 = find_exp(x_list[['drug2']], self.exp_df, 'drug2')\n",
        "        x_se = x_list['SE']\n",
        "        x_se = np.array(x_se)\n",
        "        x_se_one_hot = to_categorical(x_list['SE'], num_classes=963) # one hot encoding\n",
        "\n",
        "        x1 = np.array(x1).astype(float)\n",
        "        x2 = np.array(x2).astype(float)\n",
        "\n",
        "        return x1, x2, x_se, x_se_one_hot\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indexes = self.indexes[idx*self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x.iloc[indexes]\n",
        "        batch_y = self.y[indexes]\n",
        "\n",
        "        x1, x2, x_se, x_se_one_hot = self.__data_generation__(batch_x)\n",
        "        return [x1, x2, x_se, x_se_one_hot], batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TDVccBm94L0c"
      },
      "outputs": [],
      "source": [
        "def mean_predicted_score(true_df, predicted_y, with_plot=True):\n",
        "    test_pred_result = pd.concat([true_df.reset_index(drop=True), pd.DataFrame(predicted_y, columns=['predicted_score'])], axis=1)\n",
        "\n",
        "    if (with_plot):\n",
        "        sns.boxplot(x='label', y='predicted_score', data=test_pred_result[['label','predicted_score']])\n",
        "        plt.show()\n",
        "\n",
        "    return test_pred_result\n",
        "\n",
        "def calculate_test_performance(predicted_scores_df):\n",
        "    uniqueSE = predicted_scores_df.SE.unique()\n",
        "\n",
        "    dfDict = {elem : pd.DataFrame for elem in uniqueSE}\n",
        "\n",
        "    for key in dfDict.keys():\n",
        "        dfDict[key] = predicted_scores_df[:][predicted_scores_df.SE == key]\n",
        "\n",
        "    se_performance = pd.DataFrame(columns=['Side effect no.','SN','SP','PR','AUC','AUPR'])\n",
        "    for se in uniqueSE:\n",
        "        df = dfDict[se]\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(df.label, df.predicted_label).ravel()\n",
        "\n",
        "        auc = roc_auc_score(1-df.label, df.predicted_score)\n",
        "        aupr = average_precision_score(1-df.label, df.predicted_score)\n",
        "\n",
        "        temp_df = pd.DataFrame({'Side effect no.':se, \\\n",
        "                                'SN':tp/(tp+fn), 'SP':tn/(tn+fp), 'PR':tp/(tp+fp), 'AUC':auc, 'AUPR':aupr}, index=[0])\n",
        "        se_performance = pd.concat([se_performance, temp_df], axis=0)\n",
        "\n",
        "    return se_performance\n",
        "\n",
        "def calculate_predicted_label_ver3(predicted_score_df, optimal_thr, se_col_name='SE', threshold_col_name='optimal_thr'):\n",
        "    temp_thr = pd.DataFrame(optimal_thr.iloc[:, -7:-2].mean(axis=1), columns=[threshold_col_name])\n",
        "\n",
        "    thr = pd.concat([optimal_thr['SE'], temp_thr], axis=1)\n",
        "\n",
        "    merged = pd.merge(predicted_score_df, thr, left_on='SE', right_on=se_col_name, how='left')\n",
        "    merged['predicted_label'] = merged['predicted_score'] < merged[threshold_col_name]\n",
        "    merged.predicted_label = merged.predicted_label.map(int)\n",
        "    merged['gap'] = merged['predicted_score'] - merged[threshold_col_name]\n",
        "    merged.gap = merged.gap.map(abs)\n",
        "    test_perf = merged[['drug1','drug2','SE','label','predicted_label','predicted_score','gap']]\n",
        "    return test_perf, thr\n",
        "\n",
        "    return test_prediction_predicted_label_df, test_prediction_perf_df, thr\n",
        "def Find_Optimal_Cutoff(target, predicted):\n",
        "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "    i = np.arange(len(tpr))\n",
        "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
        "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
        "\n",
        "    return list(roc_t['threshold'])\n",
        "def cal_performance(predicted_scores_df):\n",
        "    uniqueSE = predicted_scores_df.SE.unique()\n",
        "\n",
        "    dfDict = {elem : pd.DataFrame for elem in uniqueSE}\n",
        "\n",
        "    for key in dfDict.keys():\n",
        "        dfDict[key] = predicted_scores_df[:][predicted_scores_df.SE == key]\n",
        "\n",
        "    se_performance = pd.DataFrame(columns=['Side effect no.','median_pos', 'median_neg', 'optimal_thr','SN','SP','PR','AUC','AUPR'])\n",
        "    for se in uniqueSE:\n",
        "        df = dfDict[se]\n",
        "\n",
        "        med_1 = np.median(df[df.label == 1.0].predicted_score)\n",
        "        med_0 = np.median(df[df.label == 0.0].predicted_score)\n",
        "\n",
        "        temp_thr = (med_1 + med_0)/2\n",
        "        temp_y = df.predicted_score.apply(lambda x: 0 if x > temp_thr else 1)\n",
        "        tn, fp, fn, tp = confusion_matrix(df.label, temp_y).ravel()\n",
        "\n",
        "        optimal_thr = Find_Optimal_Cutoff(1-df.label, df.predicted_score)[0]\n",
        "        temp_y_opt = df.predicted_score.apply(lambda x: 0 if x > optimal_thr else 1)\n",
        "        tn, fp, fn, tp = confusion_matrix(df.label, temp_y_opt).ravel()\n",
        "\n",
        "        auc = roc_auc_score(1-df.label, df.predicted_score)\n",
        "        aupr = average_precision_score(1-df.label, df.predicted_score)\n",
        "\n",
        "        temp_df = pd.DataFrame({'Side effect no.':se, 'median_pos':med_1, 'median_neg':med_0, 'optimal_thr':optimal_thr, \\\n",
        "                                'SN':tp/(tp+fn), 'SP':tn/(tn+fp), 'PR':tp/(tp+fp), 'AUC':auc, 'AUPR':aupr}, index=[0])\n",
        "        se_performance = pd.concat([se_performance, temp_df], axis=0)\n",
        "\n",
        "    return se_performance\n",
        "\n",
        "# Calculate average predicted scores & relabel\n",
        "def merge_both_pairs(ori_predicted_label_df, swi_predicted_label_df, optimal_threshold, thr_col_name):\n",
        "    merge_label = pd.merge(ori_predicted_label_df, swi_predicted_label_df, left_on=['drug1','drug2','SE'], right_on=['drug2','drug1','SE'])[['drug1_x','drug2_x','SE','label_x','predicted_label_x','predicted_label_y', 'predicted_score_x', 'predicted_score_y']]\n",
        "    merge_label['mean_predicted_score'] = (merge_label.predicted_score_x + merge_label.predicted_score_y)/2\n",
        "    merge_label.rename(columns={'drug1_x':'drug1','drug2_x':'drug2', 'SE_x':'SE','label_x':'label'}, inplace=True)\n",
        "\n",
        "    merged = pd.merge(merge_label, optimal_threshold, left_on='SE', right_on='SE', how='left')\n",
        "    merged['final_predicted_label'] = merged['mean_predicted_score'] < merged[thr_col_name]\n",
        "    merged.final_predicted_label = merged.final_predicted_label.map(int)\n",
        "    merged['gap'] = merged['mean_predicted_score'] - merged[thr_col_name]\n",
        "    merged.gap = merged.gap.map(abs)\n",
        "\n",
        "    merged = merged[['drug1','drug2','SE','label','predicted_label_x','predicted_label_y', 'predicted_score_x', 'predicted_score_y', \\\n",
        "            'mean_predicted_score','final_predicted_label','gap']]\n",
        "\n",
        "    uniqueSE = merged.SE.unique()\n",
        "\n",
        "    dfDict = {elem : pd.DataFrame for elem in uniqueSE}\n",
        "\n",
        "    for key in dfDict.keys():\n",
        "        dfDict[key] = merged[:][merged.SE == key]\n",
        "\n",
        "    se_performance = pd.DataFrame(columns=['Side effect no.','SN','SP','PR','AUC','AUPR'])\n",
        "    for se in uniqueSE:\n",
        "        df = dfDict[se]\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(df.label, df.final_predicted_label).ravel()\n",
        "\n",
        "        auc = roc_auc_score(1-df.label, df.mean_predicted_score)\n",
        "        aupr = average_precision_score(1-df.label, df.mean_predicted_score)\n",
        "\n",
        "        temp_df = pd.DataFrame({'Side effect no.':se, \\\n",
        "                                'SN':tp/(tp+fn), 'SP':tn/(tn+fp), 'PR':tp/(tp+fp), 'AUC':auc, 'AUPR':aupr}, index=[0])\n",
        "        se_performance = pd.concat([se_performance, temp_df], axis=0)\n",
        "\n",
        "    return merged, se_performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8MqpednI9Y"
      },
      "source": [
        "The adverse drug reation model instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DhbI_-BfbeQm"
      },
      "outputs": [],
      "source": [
        "class ADR_model(object):\n",
        "    def __init__(self,input_drug_dim=978, input_se_dim=1, drug_emb_dim=100, se_emb_dim=100, output_dim=1, margin=1, drug_activation='elu'):\n",
        "\n",
        "        self.input_drug_dim = input_drug_dim # input drug data dimension\n",
        "        self.input_se_dim = input_se_dim  # input side effect dimension\n",
        "        self.drug_emb_dim = drug_emb_dim # drug embedding dimension\n",
        "        self.se_emb_dim = se_emb_dim # side effect embedding dimension\n",
        "        self.output_dim = output_dim # output dimension\n",
        "        self.margin = margin # margin of loss function\n",
        "        self.drug_activation = drug_activation\n",
        "\n",
        "        self.callbacks = []\n",
        "        self.model = self.build()\n",
        "\n",
        "    def build(self):\n",
        "        drug1_exp = Input(shape=(self.input_drug_dim, ))\n",
        "        drug2_exp = Input(shape=(self.input_drug_dim, ))\n",
        "\n",
        "        shared_layer = Sequential(name='drug_embed_shared') # a shared sequential layer is a layer that is reused among different places in a neural network. This shared layer consist of a dense layer followed by a batch normalizaton.It is more of like a component layer that is reused in several places.\n",
        "        shared_layer.add(Dense(units=self.input_drug_dim, activation=self.drug_activation))\n",
        "        shared_layer.add(BatchNormalization())\n",
        "\n",
        "        drug1 = shared_layer(drug1_exp)\n",
        "        drug2 = shared_layer(drug2_exp)\n",
        "\n",
        "        concat = Concatenate()([drug1, drug2]) # the output from the two layers drug1 and drug2 are concantated into a single one.\n",
        "\n",
        "        glu1 = Dense(self.input_drug_dim, activation='sigmoid', name='drug1_glu')(concat)\n",
        "        glu2 = Dense(self.input_drug_dim, activation='sigmoid', name='drug2_glu')(concat)\n",
        "\n",
        "        drug1_selected = Multiply()([drug1, glu1])\n",
        "        drug2_selected = Multiply()([drug2, glu2])\n",
        "        drug1_selected = BatchNormalization()(drug1_selected)\n",
        "        drug2_selected = BatchNormalization()(drug2_selected)\n",
        "\n",
        "        shared_layer2 = Sequential(name='drug_embed_shared2')\n",
        "        shared_layer2.add(Dense(units=self.drug_emb_dim, kernel_regularizer=l2(0.001), activation=self.drug_activation))\n",
        "        shared_layer2.add(BatchNormalization())\n",
        "\n",
        "        drug1_emb = shared_layer2(drug1_selected)\n",
        "        drug2_emb = shared_layer2(drug2_selected)\n",
        "\n",
        "        # side effect\n",
        "        input_se = Input(shape=(self.input_se_dim,))\n",
        "        se_emb = Embedding(963, output_dim=self.se_emb_dim, input_length=self.input_se_dim)(input_se)\n",
        "\n",
        "        # one-hot side effect for metric\n",
        "        input_se_one_hot = Input(shape=(963,))\n",
        "\n",
        "        # side effect mapping matrix\n",
        "        se_head = Embedding(963, output_dim=self.drug_emb_dim*self.se_emb_dim, input_length=self.input_se_dim, embeddings_regularizer=l2(0.01))(input_se)\n",
        "        se_head = Reshape((self.se_emb_dim, self.drug_emb_dim))(se_head)\n",
        "        se_tail = Embedding(963, output_dim=self.drug_emb_dim*self.se_emb_dim, input_length=self.input_se_dim, embeddings_regularizer=l2(0.01))(input_se)\n",
        "        se_tail = Reshape((self.se_emb_dim, self.drug_emb_dim))(se_tail)\n",
        "\n",
        "        # MhH & MtT\n",
        "        mh_dx = Dot(axes=(2,1))([se_head, drug1_emb])\n",
        "        mt_dy = Dot(axes=(2,1))([se_tail, drug2_emb])\n",
        "        mh_dy = Dot(axes=(2,1))([se_head, drug2_emb])\n",
        "        mt_dx = Dot(axes=(2,1))([se_tail, drug1_emb])\n",
        "\n",
        "        # || MhH + r - MtT ||\n",
        "        score1 = add([mh_dx, se_emb]) # calculating the score of (side effect + drug1) and sideeffect embeddings\n",
        "        score1 = subtract([score1, mt_dy])\n",
        "        score1 = Lambda(lambda x:K.sqrt(K.sum(K.square(x), axis=-1)))(score1)\n",
        "        score1 = Reshape((1,))(score1) # reshaping score1 to have one dimension\n",
        "\n",
        "        score2 = add([mh_dy, se_emb]) # calculating the score of (side effect + drug2) and sideeffect embeddings\n",
        "        score2 = subtract([score2, mt_dx])\n",
        "        score2 = Lambda(lambda x:K.sqrt(K.sum(K.square(x), axis=-1)))(score2)\n",
        "        score2 = Reshape((1,))(score2)\n",
        "\n",
        "        final_score = add([score1, score2]) # final score\n",
        "\n",
        "        model = Model(inputs=[drug1_exp, drug2_exp, input_se, input_se_one_hot], outputs=final_score)\n",
        "        model.compile(loss=self.custom_loss_wrapper(se_one_hot=input_se_one_hot,margin=self.margin), optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def custom_loss_wrapper(self, se_one_hot, margin): # custom loss wrapper with margin paramter 1 for our use\n",
        "        def custom_margin_loss(y_true, y_pred, se_one_hot=se_one_hot, margin=margin):\n",
        "            pos_score = (y_true*y_pred)\n",
        "            neg_score = (K.abs(K.ones_like(y_true)-y_true)*y_pred)\n",
        "\n",
        "            se_pos = K.dot(K.transpose(pos_score), se_one_hot)\n",
        "            se_neg = K.dot(K.transpose(neg_score), se_one_hot)\n",
        "\n",
        "            se_mask = K.cast(se_pos*se_neg, dtype=bool)\n",
        "\n",
        "            se_pos_score = K.cast(se_mask, dtype='float32')*se_pos\n",
        "            se_neg_score = K.cast(se_mask, dtype='float32')*se_neg\n",
        "\n",
        "            score = se_pos_score-se_neg_score+(K.ones_like(se_pos_score)*K.cast(se_mask, dtype='float32'))*margin\n",
        "            final_loss = K.sum(K.maximum(K.zeros_like(score),score))\n",
        "\n",
        "            return final_loss\n",
        "        return custom_margin_loss\n",
        "\n",
        "    def get_model_summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def set_checkpoint(self):\n",
        "        # defining a custom model checkpoint with a specific decay rate\n",
        "        checkpoint= CustomModelCheckPoint(save_path=self.model_save_path, model_name=self.model_name,\n",
        "        init_learining_rate=self.init_lr, decay_rate=self.decay_rate, decay_steps=self.decay_steps)\n",
        "        self.callbacks.append(checkpoint)\n",
        "\n",
        "    def train(self, train_data, exp_df, split_frac, sampling_size, model_save_path, model_name, init_lr=0.0001, decay_rate=0.9, decay_steps=2, batch_size=1024):\n",
        "        self.model_save_path = model_save_path\n",
        "        self.model_name = model_name\n",
        "        self.init_lr = init_lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.callbacks = []\n",
        "        self.set_checkpoint()\n",
        "\n",
        "        optimal_threshold = pd.DataFrame(np.array(range(0,len(train_data.SE.unique()))), columns=['SE'])\n",
        "\n",
        "        for n in range(sampling_size):\n",
        "            print(n, ' Sample =======')\n",
        "            cv_test = train_data.groupby(['SE', 'label']).apply(pd.DataFrame.sample, frac=split_frac)\n",
        "            cv_test_x = cv_test.reset_index(drop=True).iloc[:,:3]\n",
        "            cv_test_y = cv_test.reset_index(drop=True).iloc[:,-1]\n",
        "\n",
        "            cv_train_data_rest = pd.concat([train_data, cv_test]).drop_duplicates(keep=False, inplace=False)\n",
        "            cv_train_x = cv_train_data_rest.iloc[:,:3]\n",
        "            cv_train_y = cv_train_data_rest.iloc[:,3]\n",
        "            print('Cross validation train, test dataset size: ', cv_train_x.shape, cv_test_x.shape)\n",
        "\n",
        "            cv_train_gen = CustomDataGenerator(cv_train_x, cv_train_y.values, batch_size=self.batch_size, exp_df=exp_df)\n",
        "            cv_test_gen = CustomDataGenerator(cv_test_x, cv_test_y.values, batch_size=self.batch_size, exp_df=exp_df, shuffle=False)\n",
        "\n",
        "            steps_per_epoch = cv_train_x.shape[0] // self.batch_size // 10\n",
        "\n",
        "            self.model.fit_generator(generator=cv_train_gen, steps_per_epoch=steps_per_epoch, validation_data=cv_test_gen,epochs=10, verbose=0, shuffle=True, callbacks=self.callbacks)\n",
        "\n",
        "            cv_test_pred_y = self.model.predict_generator(generator=cv_test_gen)\n",
        "\n",
        "            cv_test_prediction_scores = mean_predicted_score(cv_test, cv_test_pred_y, with_plot=False)\n",
        "            cv_test_prediction_perf = cal_performance(cv_test_prediction_scores)\n",
        "            print('AUC: {:.3f}, AUPR: {:.3f}'.format(cv_test_prediction_perf.describe().loc['mean']['AUC'], cv_test_prediction_perf.describe().loc['mean']['AUPR']))\n",
        "\n",
        "            optimal_threshold = pd.concat([optimal_threshold, pd.DataFrame(cv_test_prediction_perf.optimal_thr).reset_index(drop=True)], axis=1)\n",
        "\n",
        "        self.optimal_threshold = optimal_threshold\n",
        "        self.history = self.model.history\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        self.model.save(self.model_save_path+'final_{}.h5'.format(self.model_name))\n",
        "        print('Model saved === ')\n",
        "\n",
        "    def load_model(self, model_load_path, model_name, threshold_name):\n",
        "        self.model.load_weights(model_load_path+model_name)\n",
        "        self.optimal_threshold = pd.read_csv(model_load_path+threshold_name, index_col=0)\n",
        "\n",
        "    def predict(self, x, exp_df, batch_size=1024): # returns the predicted drug label\n",
        "        y = np.zeros(x.shape[0])\n",
        "\n",
        "        test_gen = CustomDataGenerator(x, y, batch_size=batch_size, exp_df=exp_df, shuffle=False)\n",
        "        pred_y = self.model.predict_generator(generator=test_gen)\n",
        "        predicted_result = mean_predicted_score(pd.concat([x, pd.DataFrame(y, columns=['label'])], axis=1), pred_y, with_plot=False)\n",
        "        predicted_label, _ = calculate_predicted_label_ver3(predicted_result, self.optimal_threshold)\n",
        "        predicted_label = predicted_label[['drug1','drug2','SE','predicted_label','predicted_score']]\n",
        "        return predicted_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS0oLjeOXjxb"
      },
      "source": [
        "Loading the pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oJNa11M67v4o"
      },
      "outputs": [],
      "source": [
        "adr_model = ADR_model()\n",
        "try:\n",
        "    adr_model.load_model(model_load_path='./trained_weights/', model_name='adr_model_weights.h5', threshold_name='adr_prediction_threshold.csv')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B2233eXS8rtQ"
      },
      "outputs": [],
      "source": [
        "ts_exp = pd.read_csv('./data/twosides_predicted_expression_scaled.csv')\n",
        "drug_info = pd.read_csv('./data/twosides_drug_info.csv', index_col=0)\n",
        "drug_info_dict = dict(zip(drug_info.pubchemID, drug_info.name))\n",
        "side_effect_info = pd.read_csv('./data/twosides_side_effect_info.csv', index_col=0)\n",
        "side_effect_dict = dict(zip(side_effect_info.SE_map, side_effect_info['Side Effect Name']))\n",
        "\n",
        "se_UMLS_id = dict(zip(side_effect_info.SE_id, side_effect_info.SE_map)) # drug id to drug name and side effect id to sideffect name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNQWdX7vXoru"
      },
      "source": [
        "PubMedId of two drugs and  UMLS concept unique identifier of a side effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TFcFISOw97xL"
      },
      "outputs": [],
      "source": [
        "drug1_cid = 206\n",
        "drug2_cid = 214\n",
        "side_effect_UMLS_CUI = \"C0001546\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LOt3eCnV98P2"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  side_effect_type = se_UMLS_id[side_effect_UMLS_CUI]\n",
        "except:\n",
        "  print('Side effect type not exist in TWOSIDES')\n",
        "\n",
        "temp_df = pd.DataFrame({'drug1':drug1_cid, 'drug2':drug2_cid, 'SE':side_effect_type}, index=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw8M01S97heX"
      },
      "source": [
        "Predictions:-\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8RAy6el9-xJ"
      },
      "outputs": [],
      "source": [
        "ts_drug_list = ts_exp.pubchem.values.tolist()\n",
        "if (temp_df.drug1.values not in ts_drug_list) | (temp_df.drug2.values not in ts_drug_list):\n",
        "  print('Drug cannot be found')\n",
        "else: # if found\n",
        "  predicted_label = adr_model.predict(temp_df, exp_df=ts_exp)\n",
        "  print()\n",
        "  print(\"Thank you for running this notebook\")\n",
        "  print('Drug 1: ', drug_info_dict[drug1_cid], ', Drug 2: ', drug_info_dict[drug2_cid])\n",
        "  print('Side effect name: ', side_effect_dict[side_effect_type])\n",
        "  print('Predicted score: ', predicted_label.predicted_score[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print Modal summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(adr_model.get_model_summary())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
